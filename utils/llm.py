from openai import OpenAI

class LLMHandler:
    def __init__(self,
                 model_name: str,
                 temperature: float = 0.7,
                 inference_endpoint: str = "",
                 api_key: str = "NONE"):
        """
        Constructor to initialize the LLMHandler.
        
        :param model_name: Name of the language model being used.
        :param temperature: Controls the randomness of the output.
        """
        self.model_name = model_name
        self.temperature = temperature

        # Initialize OpenAI API
        if inference_endpoint == "":
            self.api_client = OpenAI(
                api_key=api_key,
            )
        else:
            self.api_client = OpenAI(
                base_url="http://localhost:8000/v1",
                api_key=api_key,
            )

    def __llm_inference(self, prompt: str) -> str:
        """
        Private method to perform LLM inference.
        
        :param prompt: Text input for the language model.
        :return: Response generated by the model (simulated).
        """
        # Simulated response (instead of a real model call)
        response = self.api_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    stream=False,
                    temperature=self.temperature,
                    top_p=0.5,
                    frequency_penalty=0.5
            )
        
        # return f"[{self.model_name} @ {self.temperature}] Response to: {prompt}"
        return response.choices[0].message.content

    def query(self, prompt: str) -> str:
        """
        Public method that uses the private inference method.
        
        :param prompt: Text input for the model.
        :return: Model response.
        """
        return self.__llm_inference(prompt)

